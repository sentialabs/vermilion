# Embedding scripting in Bloomreach content

This is a working demonstration project / proof of concept that shows how you can create your own scripting language parser using the Antlr4 tool and implement the evaluation of simple code fragments inside of document content when the webpagee is rendered.


So as that you don't have to find out everything for yourself, I will in this readme explain parts of the what, the how and somethimes even the why. In addition I have written a high level overview blog entry that explains things in a more general manner. See here:

xxx

## Running the project
As usual with Bloomreach projects you can get it up and running with the cargo run:

```
   mvn clean package
   mvn -P cargo.run
```

Point your browser to http://localhost:8080/site and /cms for the web application (password is admin/admin).

Directly in the homepage is a document that uses embedded scripting.


# Some information regarding Antlr4

Here are some resourcs regarding Atlr4

https://tomassetti.me/antlr-mega-tutorial/

https://github.com/antlr/grammars-v4

https://alexecollins.com/antlr4-and-maven-tutorial/



## Lexer and grammar
The actual definition of a language syntax usually comes in two parts: 
- the *lexer* rules define which combinations of characters when taken together as a unit have some meaning in the language (usually such a construct is called a *token*).
- the *grammar* rules define what combinations of tokens are valid.

Note that neither lexical nor grammatical rules say what a particular combination of characters is supposed to *mean*. To give meaning, the creator of the language must add behaviour to the detected rules.

To give an example, lexical rules may define that 'a' is an identifer, '2' is a constant and '+' is the plus operator. A grammar rule maye determine that "a+2" is a valid syntax. When  this expression is parsed, at some point a callback function is called for the plus operator, the identifier 'a' and the constant '2'. 

It is however you, the creator of the language, who has to extract the value of 'a' from somewhere, convert the constant '2' to another value, add these together and return the result.

## Adding meanningful action to detected grammar rules
With antlr4, you write the lexer and grammar rules into two files in a format that the tool can understand, run the antlr4 generator, and this will output a set of Java source files that together with the antlr4 runtime library form a parser for the language.

When the parser parses some input code the result is a *parse tree*. When you subsequently *walk* this tree with one of the Parser objects that were also generated by antlr4, the various methods in the Parser are called for the various nodes in the tree. The information at what point in the tree we find ourselves is contained in a *contex* object.

Your job as a creator of the language, apart from writing the lexer and grammar for the generating process, is to extend one of the generated Parser types and fill in what should happen for various nodes in the parse three. You extract information from the context provided by the tree walker, and together with the knowledge that you are in a certain node type (because of the callback method you are overriding) you have all you need to implement a meaningful behaviour for this rule such finding values for 'a' and '2' (from the context) and adding these (you are in the 'plus' callback method).

## Details of the lexer
Antlr4 defines the various sequences of characters that together make up one token more or less in the same way that regular expressions are written down. A tilde ~ means negation, square brackets denote sets (you can also use ranges), dot, star, question mark and plus mean the same as in a regular expression and so on.

Since in this case we are using an island grammar, the lexer definition contains two differents set of lexical rules for the two different modes. 

The first mode (sea) is implicitly the default mode. The calculation mode is activated when the default parser mode encounters a boundary marker token. The boundary marker token is named NUMBERCALCOPEN. The mode switch is engaged by ways of the  ```pushMode(NUMBERCALC)``` you see in the line defining the marker.

The other mode is named NUMBERCALC and it also defines a boundary marker, named NUMBERCALCCLOSE. When that marker is encountered while in NUMBERCALC mode, the  ```popMode``` switches lexical analys back to the default mode.

The lexical structure of the default mode is very uncomplicated: all characters encounterd in that mode ar added to one single token named STATICTEXT. 

The definition of the STATTICTEXT token effectively says "any sequence of characters up to an '${*' marker should be considered STATICTEXT (in other words, not embedded script). 

The actual definition of the STATICTEXT token is a bit more complicated because we want to allow for a '$' that  is not followed by a '{' and also for a '${' that is not followed by a '\*'. Only when all three characters are encountered consecutively should this count as a marker for the start of an embedded expression.

So the there are two tokens in the default mode: the NUMBERCALCOPEN marker token, and everything that is not such a marker.

As for lexical analysis in the calculation mode, a bit more is happening there. Any single character that has a meaning in our syntax is given a name which will be used when defining grammar rules. The DIGIT and ALFA fragments are used in the definition of CONSTANT IDENTIFIER.

The lexical rule for CONSTANT says: a constant is a sequence of at least one DIGIT, possibly folllowed by a DOT and another sequence of at least one DIGIT.

The rule for IDENTIFIER allows for concatenating names with dots in between, where a name is allowed to begin with a letter or an underscore, but not with a number. 
Note that nothing is said about identifiers being case sensitive or not.  This is in fact a detail that will be settled when we will be using identifiers to obtain a value.

Finally the WS token definition says "if you encounter any space, tab, carriage return or newline character do not pass this token on to the parser".

## Details of the grammar
The ```options``` line tells antlr4 that this grammar expects tokens as defined by the lexer named "NumberLexer"

Then the grammar  starts defining syntax rules. 

At the outermost level, every sequence of tokens that is presented to the parser is matched against the "document" rule.

The document rule says that we can expect a sequence of things that are either "text" or "calculation". The sequence is allowed to be totally empty.

A "text" thing is simply a STATICTEXT token. The 'statictext' following the hash sign '#' is not actually a comment but tells antlr4 how we would like the method for this rule to be named in the generated Java code. When implementing what our scripting language should be doing, we will override the generated ```statictext``` method, extract the string that is the actual STATICTEXT, and do something useful with it (such as copying unchanged to the output). 

A "calculation" thing is an "expression" between an opening and closing token, and we want to name the  coresponding method "result".

The definition for the "expression" rule says that an expression can be one of five different sequences of tokens. Also notice that some of the definitions refer recursively to "expression". This has to do with the recursive descent aspect of parsing code.  In this way you can parse for instance ```((2+3) * 4)``` by allowing "2+3" to be an expression, while "(2+3)" is another expression which in turn is part of still another expression of which "*" and "4" are the other parts.


## Generating the lexer, parser and supporting stuff
The files describing the lexical and grammatical rules of the language are placed at a very specific location the ```calc``` project:
```
  calc/src/main
    |
    antlr4
      |
      nl
        |
        dimario
         |
         numbercalc
           NumberLexer.g4
           NumberParser.g4
```

The actual generation process is performed by running
```
   java -cp /opt/antlr-4.7.2/antlr-4.7.2-complete.jar org.antlr.v4.Tool
```
first on the lexer definition. This will create amongst other things a *.tokens file which is needed when running the same command on the grammar. Note that your mileage may vary with respect to the precise location and version of the antlr jar.

However, generating the Java code for the parser by hand is not very relevant unless you want to study what happens when you tweak the definitions. Instead, we use the ```antlr4-maven-plugin``` and run the generation as part of the build. For details, see the pom.xml of the calc project. It  is set up to generate base classes for both the visitor and the listener manifestation of the parser. In the project only the visitor is used.

Because of the specific location where the *.g4 files are located,  the generated Java code will be placed in a package named ```nl.dimario.numbercalc``` The classnames used are derived from the names given on the first lines of both files.
When the build process generates the source files it places them under ```target\generated-sources\antlr4\``` . And because this is a default configuration for Maven, it includes this source directory automatically when executing the compile fase of the build.

As a beside, when you first check out the project and import it into your IDE, you'll get a lot of Java syntax errors because the base classes that other code extends are not yet present. The errors should disappear once you have run a Maven build. This creates the missing base classes and your IDE is probably smart enough to figure out that it should look in the target/generated-sources directory for additional source code. If not, you must tweak your IDE project settings to include the generated sources.

## Implementing behaviour for the language rules.
For this part of the technical explanation, please direct your attention to 
```
  calc/src/main/java/nl/dimario/numbercalc/
```
This is where the interesting stuff happens. At the center of it all is ```NumberRenderer``` . It has a ```render()``` method that takes a parsed tree as it input and then dives into the ```visit``` method of the generated superclass. The visit method starts walking the parse three and calling relevant methods when it encounters the various types of grammatical syntax structures that have been transformed into a parse tree. 

For instance, when it encounters a node that represents a ```statictext``` grammatical rule, it will call the ```visitStatictext()``` method and pass along a context object holding information about the particulars of this statictext occurence.

Our NumberRenderer overrides the method, extracts the string that was interpreted as a STATICTEXT token, and adds it to the output buffer. The agreement about static text is that we would pass it along unchanged from input to output and that is exactly what takes place here.

To ensure that parsing the token tree continues we then let the base class take over and do its thing for static text.

When the walk along the parse tree sees a node for a "constant" syntax rule, it calls ```visitConstant()``` again with a context object that has information about this particular constant. In this case, we cannot simply add the constant to the output buffer because the constant is part of an expression and the value that it has must be used in the evaluation of the expression. So we get the string that was interpreted as a CONSTANT token, transform it to a data type that is usable in calculations and return the value. In this case, we don't call the base class to further deal with the CONSTANT, because we have already done all that was necessary.

Similar, when an ```identifier``` occurs in the tree, our override method extracts the string that is the identifier from the context and then uses it to look up a value in a Map. Where does this Map come from and how does it get its values? I will explain this later.

When the treewalk sees an expression of the form BRACEOPEN whatever BRACECLOSE it calls the ```visitBraces()``` method, as we instructed it to do in the grammar definition. Here, we know that whatever the braces surround must be an ```expression``` so we extract it from the context and let recursion deal with it.

The rules for multiplying or dividing call the ```visitMultdiv()``` method, and the rules for addition and subtraction call the ```visitAddsub()``` method. Here we must do some gymnastics because the values that we perform the basic operations on can be either integer or floating point. In order not to muddle the issue I have removed the gory details dealing with implicit conversions to a util class. The parser method merely extracts the values that the operation is performed on together with the information whether we need to multiply or divide in Multdiv() or whether me must add or subtract in Addsub().

I left the highest level syntax rule for the last: the result. The result rule is detected for the whole sequence of tokens that we encounter between a NUMBERCALCOPEN and a NUMBERCALCCLOSE token. This sequence of tokens is assumed to be an expression and thus after invoking the rule for 'expression' we are left with a value, which is the result of all arithmatic that went on in the expression.

What to do with this value? Actually, the whole purpose of this excercise was to replace an expression in the input with its calculated value in the output so this is exactly what we do: the numerical value is converted to String and appended to the output buffer. In the process, the two marker tokens are discarded and thus do not show up in the output.

Nog uitleggen:
Map met variabelen
Hoe omgaan met fouten
Hoe frot ik dit in Bloomreach