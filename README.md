# Embedding scripting in Bloomreach content

This is a working demonstration project / proof of concept that shows how you can create your own scripting language parser using the Antlr4 tool and implement the evaluation of simple code fragments inside of document content when the webpagee is rendered.


So as that you don't have to find out everything for yourself, I will in this readme explain parts of the what, the how and somethimes even the why. In addition I have written a high level overview blog entry that explains things in a more general manner. See here:

xxx

## Running the project
As usual with Bloomreach projects you can get it up and running with the cargo run:

```
   mvn clean package
   mvn -P cargo.run
```

Point your browser to http://localhost:8080/site and /cms for the web application (password is admin/admin).

Directly in the homepage is a document that uses embedded scripting.


# Some information regarding antlr4

Here are some resourcs regarding antlr4

https://tomassetti.me/antlr-mega-tutorial/

https://github.com/antlr/grammars-v4

https://alexecollins.com/antlr4-and-maven-tutorial/



## Lexer and grammar
The actual definition of a language syntax usually comes in two parts: 
- the *lexer* rules define which combinations of characters when taken together as a unit have some meaning in the language (usually such a construct is called a *token*). In this way you define keywords,  constant values, variable names, operators and so on.
- the *grammar* rules define what combinations of tokens are valid. In this way you define for instance what a statement is, what a block of statements is, how parenthesis are used, in what order operators, constants and variable names may exist and so on.

Note that neither lexical nor grammatical rules say what a particular combination of characters is supposed to *mean*, they just tell what constitutes a lexically and syntactically valid sequence of characters. To have the laguage actually do something, the creator of the language must add meaning and behaviour to the various combinations of tokens that are valid according to the grammar rules.
 
To give an example, lexical rules may define that 'a' is an identifer, '2' is a constant and '+' is the plus operator. A grammar rule maye determine that "a+2" satisfies the syntax rule for *expression*. When this fragment of code is executed, at some point the function (lilekly called __expression__) is called with arguments for the plus operator, the identifier 'a' and the constant '2'. 

It is you, the creator of the language, who has to intervene in the execution of the expression() function.  You must extract the value of 'a' from somewhere, convert the constant '2' to another value, add these values together because of the plus operator and then somehow return the resulting value.

## Adding meanningful actions to detected grammar rules
With antlr4, you write the lexer and grammar rules into two files in a format that the tool can understand, run the antlr4 generator, and this will output a set of Java source files that together with the antlr4 runtime library form a parser for the language.

When the parser parses some input code the result is a *parse tree*. When you subsequently *walk* this tree with one of the Parser objects that were also generated by antlr4, the various methods in the Parser are called for the various nodes in the tree. The information at what point in the tree we find ourselves is contained in a *contex* object.

Your job as a creator of the language, apart from writing the lexer and grammar for the generating process, is to extend one of the generated Parser types and fill in what should happen for various nodes in the parse three. You extract information from the context provided by the tree walker, and together with the knowledge that you are in a certain node type (because of the callback method you are overriding) you have all you need to implement a meaningful behaviour for this rule such finding values for 'a' and '2' (from the context) and adding these (you are in the 'plus' callback method).

## Details of the lexer
You can find the source code for the lexer here:
```
  calc/src/main/antlr4/nl/dimario/numbercalc/NumberLexer.g4
```

Antlr4 defines the various sequences of characters that together make up one token more or less in the same way that regular expressions are written down. A tilde ~ means negation, square brackets denote sets (you can also use ranges), dot, star, question mark and plus mean the same as in a regular expression and so on.

Since in this case we are using an island grammar, the lexer definition contains two differents set of lexical rules for the two different modes. 

The first mode (sea) is implicitly the default mode. The calculation mode is activated when the default parser mode encounters a boundary marker token. The boundary marker token is named NUMBERCALCOPEN. The mode switch is engaged by ways of the  ```pushMode(NUMBERCALC)``` you see in the line defining the marker.

The other mode is named NUMBERCALC and it also defines a boundary marker, named NUMBERCALCCLOSE. When that marker is encountered while in NUMBERCALC mode, the  ```popMode``` switches lexical analys back to the default mode.

The lexical structure of the default mode is very uncomplicated: all characters encounterd in that mode ar added to one single token named STATICTEXT. 

The definition of the STATTICTEXT token effectively says "any sequence of characters up to an '${*' marker should be considered STATICTEXT (in other words, not embedded script). 

The actual definition of the STATICTEXT token is a bit more complicated because we want to allow for a '$' that  is not followed by a '{' and also for a '${' that is not followed by a '\*'. Only when all three characters are encountered consecutively should this count as a marker for the start of an embedded expression.

So the there are two tokens in the default mode: the NUMBERCALCOPEN marker token for the mode switch, and the STATICTEXT token for everything that is not such a marker.

As for lexical analysis in the calculation mode, a bit more is happening there. Any single character that has a meaning in our syntax is given a name which will be used when defining grammar rules. The DIGIT and ALFA fragments are used in the definition of CONSTANT IDENTIFIER. Fragments are in themselves not a whole token but they aid in defining them. The concept of fragments is specific for the antlr4 tool.

The lexical rule for CONSTANT says: a constant is a sequence of at least one DIGIT, possibly folllowed by a DOT and another sequence of at least one DIGIT.

The rule for IDENTIFIER allows for concatenating names with dots in between, where a name is allowed to begin with a letter or an underscore, but not with a digit. This makes sense because otherwise we would have a hard time knowing if something is a constant or an identifier. Note that nothing is said about identifiers being case sensitive or not.  This is in fact a detail that will be settled when we will be using identifiers to obtain a value.

Finally the WS token definition has a *skip* directive that says "if you encounter any space, tab, carriage return or newline character do not pass this on to the parser". WS  stands for "whitespace".

## Details of the grammar
You can find the source code for the grammar here:
```
  calc/src/main/antlr4/nl/dimario/numbercalc/NumberParser.g4
```
The ```options``` line tells antlr4 that this grammar expects tokens as defined by the lexer named "NumberLexer"

Then the grammar  starts defining syntax rules. 

At the uppermost level, every sequence of tokens that is presented to the parser is matched against the "document" rule.

The document rule says that we can expect a sequence of syntax constructions that are either "text" or "calculation". In addition, the sequence is allowed to be totally empty.

A "text" construct is simply a sequence of exactly one STATICTEXT token all by itself. The 'statictext' following the hash sign '#' is not actually a comment but tells antlr4 how we would like the method for this rule to be named in the generated Java code. When implementing what our scripting language should be doing, we will override the generated ```statictext``` method, extract the string that is the actual STATICTEXT, and do something useful with it (such as copying unchanged to the output). 

A "calculation" thing is an "expression" between an opening and closing token, and we want to name the  coresponding method "result".

The definition for the "expression" rule says that an expression can be one of five different sequences of tokens. Also notice that some of the definitions refer recursively to "expression". This has to do with the recursive descent aspect of parsing code.  In this way you can parse for instance ```((2+3) * 4)``` by allowing ```2+3``` to be an expression, while ```(2+3)``` is another expression which in turn is part of still another expression of which ```*``` and ```4``` are the other parts.


## Generating the lexer, parser and supporting stuff
The actual generation process is performed by running
```
   java -cp /opt/antlr-4.7.2/antlr-4.7.2-complete.jar org.antlr.v4.Tool
```
first on the lexer definition. This will create amongst other things a *.tokens file which is needed when running the same command on the grammar. Note that your mileage may vary with respect to the precise location and version of the antlr jar.

However, generating the Java code for the parser by hand is not very relevant unless you want to study what happens when you tweak the definitions. Instead, we use the ```antlr4-maven-plugin``` and run the generation as part of the build. For details, see the pom.xml of the calc project. It  is set up to generate base classes for both the visitor and the listener manifestation of the parser. In the project only the visitor is used.

Because of the very specific location where the *.g4 files are located,  the generated Java code will be placed in a package named ```nl.dimario.numbercalc``` The classnames used are derived from the names given on the first lines of both files. When the build process generates the source files it places them under ```target\generated-sources\antlr4\``` . This is a default configuration location for Maven, so it includes this source directory automatically when executing the compile fase of the build.

As a beside, when you first check out the project and import it into your IDE, you'll get a lot of Java syntax errors because the base classes that other code extends are not yet present. The errors should disappear once you have run a Maven build. The build creates the missing base classes and your IDE is probably smart enough to figure out that it should look in the target/generated-sources directory for additional source code. If not, you must tweak your IDE project settings to include the generated sources.

## Implementing behaviour for the language syntax rules.
For this part of the technical explanation, please direct your attention to 
```
  calc/src/main/java/nl/dimario/numbercalc/NumberRenderer.java
```
This is where the interesting stuff happens. NumberRenderer has a ```render()``` method that takes a parsed tree as its input and then dives into the ```visit``` method of the generated base class. The visit method starts walking the parse three and calling relevant methods when it encounters the various types of grammatical syntax structures that have been transformed into a parse tree. 

For instance, when it encounters a node that represents a ```statictext``` grammatical rule, it will call the ```visitStatictext()``` method and pass along a context object holding information about the particulars of this statictext occurence.

Our NumberRenderer overrides the method, extracts the string that was interpreted as a STATICTEXT token, and adds it to the output buffer. The agreement about static text is that we would pass it along unchanged from input to output and that is exactly what takes place here.

To ensure that parsing the token tree continues we then let the base class take over and do its thing.

When the walk along the parse tree sees a node for a "constant" syntax rule, it calls ```visitConstant()``` again with a context object that has information about this particular constant. In this case, we cannot simply add the constant to the output buffer because the constant is part of an expression and the value that it has must be used in the evaluation of the expression. So we get the string that was interpreted as a CONSTANT token, transform it to a data type that is usable in calculations and return the value. In this case, we don't call the base class to further deal with the CONSTANT, because we have already done all that was necessary.

Similar, when an ```identifier``` occurs in the tree, our override method extracts the string that is the identifier from the context and then uses it to look up a value in a Map. Where does this Map come from and how does it get its values? I will explain this later on.

When the treewalk sees an expression of the form BRACEOPEN whatever BRACECLOSE it calls the ```visitBraces()``` method, as we instructed it to do in the grammar definition. Here, we know that whatever the braces surround must be an ```expression``` so we extract it from the context and let recursion deal with it.

The rule for multiplying or dividing calls the ```visitMultdiv()``` method, and the rule for addition and subtraction calls the ```visitAddsub()``` method. Here we must perform some gymnastics because the values used in these basic operations can be either integer or floating point and we must take care to convert the operands to either floating point or integer values before using them. In order not to muddle the issue I have removed the gory details dealing with implicit conversions to a utility class. The parser method merely extracts the values that the operation is performed on together with the information whether we need to multiply or divide in Multdiv() or whether me must add or subtract in Addsub().

I left the highest level syntax rule for the last: the result. The result rule is detected for the whole sequence of tokens that we encounter between a NUMBERCALCOPEN and a NUMBERCALCCLOSE token. This sequence of tokens is assumed to be an expression and thus after invoking the rule for 'expression' we are left with a value, which is the result of all arithmatic that went on inside of the open and close markers.

What to do with this value? Actually, the whole purpose of this excercise was to replace an expression in the input with its calculated value in the output so this is exactly what we do: the numerical value is converted to String and appended to the output buffer. In the process, the two marker tokens are discarded and thus do not show up in the output.

## Adding data from an external source to the mix
You were left wondering how to obtain the values that we must fill in whenever we encounter the name of a variable, a.k.a. identifier. The mechanism in itself is pretty straight forward: we assume that we have a ```Map<String,Number>``` that we use to lookup a value for any variable name we encounter. This does not explain how the values get in the map, nor how we know what names to use or what they mean. 

You probably won't like the answer, but the long and short of it is: it depends.

The most obvious place to get data from would be an external service. The service could be a REST like server, or perhaps an interface to a database. Or, god forbid but shit happens, some unfortunate creature that reads Excel spreadsheets for a living. Or all of the above.

The general idea here is that you *somehow* obtain data from whatever source is relevant for your project, and then pass it on to the NumberRenderer in the form of a Map filled with Numbers. Obviously whoever is responsible for adding embedded expressions to the content must know what kind of data may appear in the Map as a result of the various extraneous shenanigans, and more precisely he or she should know what names to use for the variables (and what the values for these names mean).

There is a little bit of practical support I can offer: when the data is returned by the service as a JSON object, you could walk this object recursively and place any attribute value in the Map, perhaps using a path-like approach where you concatenate the names of the parent object to the child attribute with a dot in between. This is what happens in JsonDataSource which I included as an example.

Of course you would be left with figuring out what to do with arrays (hint: allow variable names to have square brackets with numbers inside them, and add all elements of the array separately with an approriate counter in their name) and what to do with values that are not numbers (hint: ignore these for the time being).

Another approach which is useful when you have to query a service specifically for some value or other by name: before rendering, scan the input and collect the names of all variables used. Then submit this list of names to your external service and digest the result into the aforementioned Map for the renderer. In fact I have added ```NumberVariableScanner.java``` to the package as an illustration of how to do this.

## Dealing with syntax errors
Ook uitleggenL scriptexpander


## How to use the parser in a real live Bloomreach web application
Hoe frot ik dit in Bloomreach
